for (a in aa) {
feature_df[[paste0(a, "_prop")]] <-
str_count(feature_df$sequence, fixed(a)) /
str_length(feature_df$sequence)
}
# Splitting the data
set.seed(22)
splitting <- createDataPartition(feature_df$class, p = 0.7, list = FALSE)
train_2 <- feature_df[splitting, ]
test_2  <- feature_df[-splitting, ]
# Create test and train DF without leakage features
train_rf <- train %>% select(-sequence, -id, -AMP_status)
test_rf  <- test  %>% select(-sequence, -id, -AMP_status)
train_rf$class <- factor(train_rf$class, levels = c("nonAMP", "AMP"))
test_rf$class  <- factor(test_rf$class,  levels = c("nonAMP", "AMP"))
x_train <- train_rf %>% select(-class)
y_train <- train_rf$class
tuned <- tuneRF(
x = x_train,
y = y_train,
ntreeTry = 500,
stepFactor = 1.5,
improve = 0.01,
trace = TRUE
)
set.seed(22)
best_mtry <- tuned[which.min(tuned[, "OOBError"]), "mtry"]
# Train model using the best mtry
rf <- randomForest(
class ~ .,
data = train_rf,
ntree = 1000,
mtry = best_mtry,
classwt = c(nonAMP = 1, AMP = 3),
importance = TRUE
)
pred_class <- predict(rf, newdata = test_rf)
pred_prob  <- predict(rf, newdata = test_rf, type = "prob")[, "AMP"]
confusionMatrix(pred_class, test_rf$class)
roc_obj <- roc(
response = test_rf$class,
predictor = pred_prob,
levels = c("nonAMP", "AMP")
)
auc(roc_obj)
plot(roc_obj)
varImpPlot(rf, n.var = 15)
# Predictions
pred_class <- predict(rf, newdata = test_rf)
pred_prob  <- predict(rf, newdata = test_rf, type = "prob")[, "AMP"]
# Confusion matrix
confusionMatrix(pred_class, test_rf$class)
# ROC
roc_obj <- roc(
response = test_rf$class,
predictor = pred_prob,
levels = c("nonAMP", "AMP")
)
auc(roc_obj)
plot(roc_obj)
# Variable importance
varImpPlot(rf, n.var = 15)
length(pred_prob) == nrow(test_rf)
all(pred_prob >= 0 & pred_prob <= 1)
all(is.finite(pred_prob))
# Ways to improve the RF model
## Add additional features (Individual aa composition, di-amino acid composition (400 features!))
## Tweak model parameters
## Have a more imbalanced dataset? i.e. favouring nonAMP
aa <- c("A","C","D","E","F","G","H","I","K","L",
"M","N","P","Q","R","S","T","V","W","Y")
# Add individual amino acid proportion to the feature dataframe
for (a in aa) {
feature_df[[paste0(a, "_prop")]] <-
str_count(feature_df$sequence, fixed(a)) /
str_length(feature_df$sequence)
}
# Splitting the data
set.seed(22)
splitting <- createDataPartition(feature_df$class, p = 0.7, list = FALSE)
train_2 <- feature_df[splitting, ]
test_2  <- feature_df[-splitting, ]
# Create test and train DF without leakage features
train_rf <- train_2 %>% select(-sequence, -id, -AMP_status)
test_rf  <- test_2  %>% select(-sequence, -id, -AMP_status)
train_rf$class <- factor(train_rf$class, levels = c("nonAMP", "AMP"))
test_rf$class  <- factor(test_rf$class,  levels = c("nonAMP", "AMP"))
x_train <- train_rf %>% select(-class)
y_train <- train_rf$class
tuned <- tuneRF(
x = x_train,
y = y_train,
ntreeTry = 500,
stepFactor = 1.5,
improve = 0.01,
trace = TRUE
)
set.seed(22)
best_mtry <- tuned[which.min(tuned[, "OOBError"]), "mtry"]
# Train model using the best mtry
rf <- randomForest(
class ~ .,
data = train_rf,
ntree = 1000,
mtry = best_mtry,
classwt = c(nonAMP = 1, AMP = 3),
importance = TRUE
)
# Predictions
pred_class <- predict(rf, newdata = test_rf)
pred_prob  <- predict(rf, newdata = test_rf, type = "prob")[, "AMP"]
# Confusion matrix
confusionMatrix(pred_class, test_rf$class)
# ROC
roc_obj <- roc(
response = test_rf$class,
predictor = pred_prob,
levels = c("nonAMP", "AMP")
)
auc(roc_obj)
plot(roc_obj)
# Variable importance
varImpPlot(rf, n.var = 15)
saveRDS(rf, file = "../R/model/rf_model.rds")
runApp('R')
runApp('R')
runApp('R')
runApp('R')
runApp('R')
train$class <- factor(train$class, levels = c("nonAMP", "AMP"))
test$class  <- factor(test$class,  levels = c("nonAMP", "AMP"))
# Train a Random Forest classifier
rf <- randomForest(
class ~ .,
data = train,
ntree = 500,
importance = TRUE
)
# Predict class labels on the test set
pred_class <- predict(rf, newdata = test)
# Predict class probabilities on the test set
pred_prob <- predict(rf, newdata = test, type = "prob")[, "AMP"]
# Generate confusion matrix
rf_cm <- table(Predicted = pred_class, Actual = test$class)
train$class <- factor(train$class, levels = c("nonAMP", "AMP"))
test$class  <- factor(test$class,  levels = c("nonAMP", "AMP"))
# Train a Random Forest classifier
rf <- randomForest(
class ~ .,
data = train,
ntree = 500,
importance = TRUE
)
# Predict class labels on the test set
pred_class <- predict(rf, newdata = test)
# Predict class probabilities on the test set
pred_prob <- predict(rf, newdata = test, type = "prob")[, "AMP"]
# Generate confusion matrix
rf_cm <- table(Predicted = pred_class, Actual = test$class)
rf_cm
# Extract counts
rf_TP <- as.numeric(rf_cm["AMP", "AMP"])
rf_TN <- as.numeric(rf_cm["nonAMP", "nonAMP"])
rf_FP <- as.numeric(rf_cm["AMP", "nonAMP"])
rf_FN <- as.numeric(rf_cm["nonAMP", "AMP"])
# Metrics
rf_accuracy <- (rf_TP + rf_TN) / sum(rf_cm)
rf_sensitivity <- rf_TP / (rf_TP + rf_FN)
rf_specificity <- rf_TN / (rf_TN + rf_FP)
rf_mcc <- (rf_TP * rf_TN - rf_FP * rf_FN) /
sqrt((rf_TP + rf_FP) * (rf_TP + rf_FN) * (rf_TN + rf_FP) * (rf_TN + rf_FN))
# AUC
rf_roc_obj <- roc(test$class, pred_prob, levels = c("nonAMP", "AMP"))
rf_auc_val <- as.numeric(auc(rf_roc_obj))
# Final summary
rf_metrics <- data.frame(
Accuracy = rf_accuracy,
Sensitivity = rf_sensitivity,
Specificity = rf_specificity,
AUC = rf_auc_val,
MCC = rf_mcc
)
rf_metrics
# Input AMP Data from a fasta file
AMP_seqs <- read.fasta(file = "../Data/amps.fasta", seqtype = "AA")
# Input non-AMP DATA from a fasta file
non_AMP_seqs <- read.fasta(file = "../Data/non_amps.fasta", seqtype = "AA")
# Store sequences as a list
AMP_seq_chars <- sapply(AMP_seqs, function(x) paste(x, collapse = ""))
non_AMP_seq_chars <- sapply(non_AMP_seqs, function(x) paste(x, collapse = ""))
# Remove duplicate sequences
cat("Number of AMP sequences removed:", length(AMP_seq_chars) - length(unique(AMP_seq_chars)), "\n")
AMP_seq_chars <- unique(AMP_seq_chars)
cat("Number of Non-AMP sequences removed:", length(non_AMP_seq_chars) - length(unique(non_AMP_seq_chars)), "\n")
non_AMP_seq_chars <- unique(non_AMP_seq_chars)
# Create Dataframes for AMP and non-AMP
# AMP dataframe
AMP_df<- data.frame(
AMP_status = 1,
class = c('AMP'),
sequence = AMP_seq_chars,
stringsAsFactors = FALSE
)
# nonAMP dataframe
non_AMP_df<- data.frame(
AMP_status = 0,
class = c('nonAMP'),
sequence = non_AMP_seq_chars,
stringsAsFactors = FALSE
)
# Remove any non-standard amino acids
AMP_df <- AMP_df[!grepl("[^ACDEFGHIKLMNPQRSTVWY]", AMP_df$sequence), ]
non_AMP_df <- non_AMP_df[!grepl("[^ACDEFGHIKLMNPQRSTVWY]", non_AMP_df$sequence), ]
# Merge the two data frames, ensuring the number of nonAMPs is equal to AMP (balanced)
feature_df = rbind(AMP_df, sample_n(non_AMP_df, size=nrow(AMP_df)))
# Create some features using the Peptides package
feature_df <- feature_df %>%
mutate(
length = lengthpep(sequence),
mw = mw(sequence),
pI = pI(sequence),
hydrophobicity = hydrophobicity(sequence),
charge = charge(sequence),
amphipathicity = hmoment(sequence),
aliphatic_index = aIndex(sequence),
boman_index = boman(sequence),
hmoment = hmoment(sequence)
)
# Creature a column for IDs
feature_df <- feature_df %>%
mutate(id = sprintf("%05d", row_number())) %>%
relocate(id, .before = 1)
# Structural features added to the dataframe
# Tiny (A, C, G, S, T)
feature_df$tiny_prop <- str_count(feature_df$sequence, "[ACGST]") / str_length(feature_df$sequence)
# Small (A, B, C, D, G, N, P, S, T, V)
feature_df$small_prop <- str_count(feature_df$sequence, "[ABCDGNPSTV]")/ str_length(feature_df$sequence)
# Aliphatic (A, I, L, V)
feature_df$aliphatic_prop <- str_count(feature_df$sequence, "[AILV]")/ str_length(feature_df$sequence)
# Aromatic (F, H, W, Y)
feature_df$aromatic_prop <- str_count(feature_df$sequence, "[FHWY]")/ str_length(feature_df$sequence)
# Positive (H, K, R)
feature_df$pos_prop <- str_count(feature_df$sequence, "[HKR]")/ str_length(feature_df$sequence)
# Negative (D, E)
feature_df$neg_prop <- str_count(feature_df$sequence, "[DE]")/ str_length(feature_df$sequence)
# Charged (D, E, H, K, R)
feature_df$charged_prop <- str_count(feature_df$sequence, "[DEHKR]")/ str_length(feature_df$sequence)
# Polar (D, E, H, K, N, Q, R, S, T, Z)
feature_df$polar_prop <- str_count(feature_df$sequence, "[DEHKNQRSTZ]")/ str_length(feature_df$sequence)
# Create a list of the feature headings
key_features <- c("length", "mw", "pI", "hydrophobicity", "charge", "amphipathicity", "aliphatic_index", "boman_index", "hmoment")
key_plots <- feature_df %>%
select(class, all_of(key_features)) %>%
pivot_longer(
cols = -class,
names_to = "feature",
values_to = "value"
)
ggplot(key_plots, aes(x = value, fill = class)) +
geom_histogram(
bins = 30,
alpha = 0.6,
position = "identity"
) +
facet_wrap(~ feature, scales = "free") +
theme_bw() +
labs(
title = "Feature distributions for AMP vs nonAMP",
x = "Feature value",
y = "Count"
)
# Part 2 of features plotting: structural features
proportion_features <- c("tiny_prop", "small_prop", "aliphatic_prop", "aromatic_prop", "pos_prop", "neg_prop", "charged_prop", "polar_prop")
proportion_plots <- feature_df %>%
select(class, all_of(proportion_features)) %>%
pivot_longer(
cols = -class,
names_to = "feature",
values_to = "value"
)
ggplot(proportion_plots, aes(x = value, fill = class)) +
geom_histogram(
bins = 30,
alpha = 0.6,
position = "identity"
) +
facet_wrap(~ feature, scales = "free") +
theme_bw() +
labs(
title = "Feature distributions for AMP vs nonAMP",
x = "Feature value",
y = "Count"
)
# Create a dataframe of using only the numeric features
feature_mat <- feature_df %>%
select(where(is.numeric))
# Compute the Pearson correlation matrix
cor_mat <- cor(feature_mat, method = "pearson")
# Visualise the correlation matrix
corrplot(
cor_mat,
method = "color",
type = "upper",
tl.cex = 0.7
)
# Identify highly correlated feature pairs
# abs(correlation) > 0.9 indicates strong correlation
# abs(correlation) < 1 removes self-correlations
high_cor <- which(abs(cor_mat) > 0.9 & abs(cor_mat) < 1, arr.ind = TRUE)
# Convert highly correlated pairs into a readable data frame
high_cor_pairs <- unique(
data.frame(
feature1 = rownames(cor_mat)[high_cor[, 1]],
feature2 = colnames(cor_mat)[high_cor[, 2]],
r = cor_mat[high_cor]
)
)
high_cor_pairs
# Remove reduntant pairs
feature_df <- feature_df %>%
select(
-mw,
-hmoment,
-aliphatic_prop,
-boman_index
)
# Recreate the numeric dataframe post-reduction of redundant pairs
feature_mat_red <- feature_df %>%
select(where(is.numeric))
# Perform PCA
pca_red <- prcomp(feature_mat_red, scale. = TRUE)
# Create a dataframe of the first two principal components
pca_df_red <- data.frame(
PC1 = pca_red$x[, 1],
PC2 = pca_red$x[, 2],
class = feature_df$class
)
# Visualise the results
ggplot(pca_df_red, aes(PC1, PC2, color = class)) +
geom_point(alpha = 0.6) +
theme_bw() +
labs(title = "PCA")
# Splitting the data
set.seed(123)
#use 70% of dataset as training set and 30% as test set
train <- feature_df %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(feature_df, train, by = 'id')
# Logistic regression using just 5-10 most promising features
# Most promising features
selected_features <- c(
"length",
"charge",
"hydrophobicity",
"amphipathicity",
"aliphatic_index",
"pI"
)
# Subset training and test data to selected features
# AMP_status is the binary response variable
train_lr <- train[, c("AMP_status", selected_features)]
test_lr  <- test[,  c("AMP_status", selected_features)]
# Fit a logistic regression model
lr_model <- glm(
AMP_status ~ .,
data = train_lr,
family = binomial
)
summary(lr_model)
# Predict probabilities on the test set
prob_amp <- predict(lr_model, newdata = test_lr, type = "response")
# Class predictions (0 = nonAMP, 1 = AMP)
pred_class <- ifelse(prob_amp >= 0.5, 1, 0)
# Confusion matrix
cm <- table(
Predicted = pred_class,
Actual = test_lr$AMP_status
)
cm
# Accuracy
accuracy <- sum(diag(cm)) / sum(cm)
# Sensitivity (Recall / TPR)
sensitivity <- cm["1", "1"] / sum(cm[, "1"])
# Specificity (TNR)
specificity <- cm["0", "0"] / sum(cm[, "0"])
# MCC
TP <- as.numeric(cm["1", "1"])
TN <- as.numeric(cm["0", "0"])
FP <- as.numeric(cm["1", "0"])
FN <- as.numeric(cm["0", "1"])
mcc <- (TP * TN - FP * FN) /
sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
# ROC and AUC
roc_obj <- roc(
response = test_lr$AMP_status,
predictor = prob_amp,
levels = c(0, 1),
direction = "<"
)
auc_val <- auc(roc_obj)
plot(roc_obj, main = "Logistic Regression ROC")
# Final summary metrics
lr_metrics <- data.frame(
Accuracy = accuracy,
Sensitivity = sensitivity,
Specificity = specificity,
AUC = as.numeric(auc_val),
MCC = mcc
)
lr_metrics
train$class <- factor(train$class, levels = c("nonAMP", "AMP"))
test$class  <- factor(test$class,  levels = c("nonAMP", "AMP"))
# Train a Random Forest classifier
rf <- randomForest(
class ~ .,
data = train,
ntree = 500,
importance = TRUE
)
# Predict class labels on the test set
pred_class <- predict(rf, newdata = test)
# Predict class probabilities on the test set
pred_prob <- predict(rf, newdata = test, type = "prob")[, "AMP"]
# Generate confusion matrix
rf_cm <- table(Predicted = pred_class, Actual = test$class)
rf_cm
# Extract counts
rf_TP <- as.numeric(rf_cm["AMP", "AMP"])
rf_TN <- as.numeric(rf_cm["nonAMP", "nonAMP"])
rf_FP <- as.numeric(rf_cm["AMP", "nonAMP"])
rf_FN <- as.numeric(rf_cm["nonAMP", "AMP"])
# Metrics
rf_accuracy <- (rf_TP + rf_TN) / sum(rf_cm)
rf_sensitivity <- rf_TP / (rf_TP + rf_FN)
rf_specificity <- rf_TN / (rf_TN + rf_FP)
rf_mcc <- (rf_TP * rf_TN - rf_FP * rf_FN) /
sqrt((rf_TP + rf_FP) * (rf_TP + rf_FN) * (rf_TN + rf_FP) * (rf_TN + rf_FN))
# AUC
rf_roc_obj <- roc(test$class, pred_prob, levels = c("nonAMP", "AMP"))
rf_auc_val <- as.numeric(auc(rf_roc_obj))
# Final summary
rf_metrics <- data.frame(
Accuracy = rf_accuracy,
Sensitivity = rf_sensitivity,
Specificity = rf_specificity,
AUC = rf_auc_val,
MCC = rf_mcc
)
rf_metrics
combined <- bind_rows(
"Linear Regression" = lr_metrics,
"Random Forest" = rf_metrics,
.id = "Model"
)
combined
# Ways to improve the RF model
## Add additional features (Individual aa composition, di-amino acid composition (400 features!))
## Tweak model parameters
## Have a more imbalanced dataset? i.e. favouring nonAMP
aa <- c("A","C","D","E","F","G","H","I","K","L",
"M","N","P","Q","R","S","T","V","W","Y")
# Add individual amino acid proportion to the feature dataframe
for (a in aa) {
feature_df[[paste0(a, "_prop")]] <-
str_count(feature_df$sequence, fixed(a)) /
str_length(feature_df$sequence)
}
# Splitting the data
set.seed(22)
splitting <- createDataPartition(feature_df$class, p = 0.7, list = FALSE)
train_2 <- feature_df[splitting, ]
test_2  <- feature_df[-splitting, ]
# Create test and train DF without leakage features
train_rf <- train_2 %>% select(-sequence, -id, -AMP_status)
test_rf  <- test_2  %>% select(-sequence, -id, -AMP_status)
train_rf$class <- factor(train_rf$class, levels = c("nonAMP", "AMP"))
test_rf$class  <- factor(test_rf$class,  levels = c("nonAMP", "AMP"))
x_train <- train_rf %>% select(-class)
y_train <- train_rf$class
tuned <- tuneRF(
x = x_train,
y = y_train,
ntreeTry = 500,
stepFactor = 1.5,
improve = 0.01,
trace = TRUE
)
set.seed(22)
best_mtry <- tuned[which.min(tuned[, "OOBError"]), "mtry"]
# Train model using the best mtry
rf <- randomForest(
class ~ .,
data = train_rf,
ntree = 1000,
mtry = best_mtry,
classwt = c(nonAMP = 1, AMP = 3),
importance = TRUE
)
# Predictions
pred_class <- predict(rf, newdata = test_rf)
pred_prob  <- predict(rf, newdata = test_rf, type = "prob")[, "AMP"]
# Confusion matrix
confusionMatrix(pred_class, test_rf$class)
# ROC
roc_obj <- roc(
response = test_rf$class,
predictor = pred_prob,
levels = c("nonAMP", "AMP")
)
auc(roc_obj)
plot(roc_obj)
# Variable importance
varImpPlot(rf, n.var = 15)
