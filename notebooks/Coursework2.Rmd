---
title: "R Notebook"
output: html_notebook
---

This notebook contains the code to conduct antimicrobial peptides prediction using machine learning. The code will take a FASTA file of peptide sequences.

```{r}
# Install the necessary packages
install.packages("Peptides", dependencies=TRUE)
install.packages("protr")
install.packages("seqinr")
install.packages("stringr")
install.packages("randomForest")
install.packages("corrplot")
```

```{r}
install.packages("caret")
```

```{r}
# Load the necessary packages
library("Peptides")
library("protr")
library(seqinr)
library(dplyr)
library(stringr)
library(corrplot)
library(pROC)
library(tidyr)
library(ggplot2)
library(caret)
```

Importing and cleaning of the training dataset.

Importing the AMP dataset sourced from X. The negative datset was obtained from X using the following search criteria: . Both datasets are a FASTA file

```{r}
# Input AMP Data from a fasta file
AMP_seqs <- read.fasta(file = "./Data/amps.fasta", seqtype = "AA")

# Input non-AMP DATA from a fasta file
non_AMP_seqs <- read.fasta(file = "./Data/non_amps.fasta", seqtype = "AA")

# Store sequences as a list
AMP_seq_chars <- sapply(AMP_seqs, function(x) paste(x, collapse = ""))
non_AMP_seq_chars <- sapply(non_AMP_seqs, function(x) paste(x, collapse = ""))

# Remove duplicates
AMP_seq_chars <- unique(AMP_seq_chars)
non_AMP_seq_chars <- unique(non_AMP_seq_chars)
```

```{r}
# Create Dataframes for AMP and non-AMP

# AMP dataframe
AMP_df<- data.frame(
  AMP_status = 1,
  class = c('AMP'),
  sequence = AMP_seq_chars,
  stringsAsFactors = FALSE
)

# nonAMP dataframe
non_AMP_df<- data.frame(
  AMP_status = 0,
  class = c('nonAMP'),
  sequence = non_AMP_seq_chars,
  stringsAsFactors = FALSE
)


```

```{r}
# Remove any non-standard amino acids
AMP_df <- AMP_df[!grepl("[^ACDEFGHIKLMNPQRSTVWY]", AMP_df$sequence), ]
non_AMP_df <- non_AMP_df[!grepl("[^ACDEFGHIKLMNPQRSTVWY]", non_AMP_df$sequence), ]


# Merge the two data frames, ensuring the number of nonAMPs is equal to AMP (balanced)
feature_df = rbind(AMP_df, sample_n(non_AMP_df, size=nrow(AMP_df))) 
```

Feature creation using packages

```{r}
# Create some features using the packages

feature_df <- feature_df %>%
  mutate(
    length = lengthpep(sequence),
    mw = mw(sequence),
    pI = pI(sequence),
    hydrophobicity = hydrophobicity(sequence),
    charge = charge(sequence),
    amphipathicity = hmoment(sequence),
    aliphatic_index = aIndex(sequence),
    boman_index = boman(sequence),
    hmoment = hmoment(sequence)
  )


# Creature a column for IDs
feature_df <- feature_df %>%
  mutate(id = sprintf("%05d", row_number())) %>%
  relocate(id, .before = 1)






```

```{r}
# Additional structural features

# Tiny (A, C, G, S, T)
feature_df$tiny_prop <- str_count(feature_df$sequence, "[ACGST]") / str_length(feature_df$sequence)

# Small (A, B, C, D, G, N, P, S, T, V)
feature_df$small_prop <- str_count(feature_df$sequence, "[ABCDGNPSTV]")/ str_length(feature_df$sequence)

# Aliphatic (A, I, L, V)
feature_df$aliphatic_prop <- str_count(feature_df$sequence, "[AILV]")/ str_length(feature_df$sequence)

# Aromatic (F, H, W, Y)
feature_df$aromatic_prop <- str_count(feature_df$sequence, "[FHWY]")/ str_length(feature_df$sequence)

# Positive (H, K, R)
feature_df$pos_prop <- str_count(feature_df$sequence, "[HKR]")/ str_length(feature_df$sequence)

# Negative (D, E)
feature_df$neg_prop <- str_count(feature_df$sequence, "[DE]")/ str_length(feature_df$sequence)

# Charged (D, E, H, K, R)
feature_df$charged_prop <- str_count(feature_df$sequence, "[DEHKR]")/ str_length(feature_df$sequence)

# Polar (D, E, H, K, N, Q, R, S, T, Z)
feature_df$polar_prop <- str_count(feature_df$sequence, "[DEHKNQRSTZ]")/ str_length(feature_df$sequence)


```

Further Cleaning

```{r}
## Make sure all the columns are the right class
sapply(feature_df, class)
```

```{r}
# Create a list of the feature headings
key_features <- c("length", "mw", "pI", "hydrophobicity", "charge", "amphipathicity", "aliphatic_index", "boman_index", "hmoment")



key_plots <- feature_df %>%
  select(class, all_of(key_features)) %>%
  pivot_longer(
    cols = -class,
    names_to = "feature",
    values_to = "value"
  )

ggplot(key_plots, aes(x = value, fill = class)) +
  geom_histogram(
    bins = 30,
    alpha = 0.6,
    position = "identity"
  ) +
  facet_wrap(~ feature, scales = "free") +
  theme_bw() +
  labs(
    title = "Feature distributions for AMP vs nonAMP",
    x = "Feature value",
    y = "Count"
  )


```

```{r}

proportion_features <- c("tiny_prop", "small_prop", "aliphatic_prop", "aromatic_prop", "pos_prop", "neg_prop", "charged_prop", "polar_prop")

proportion_plots <- feature_df %>%
  select(class, all_of(proportion_features)) %>%
  pivot_longer(
    cols = -class,
    names_to = "feature",
    values_to = "value"
  )

ggplot(proportion_plots, aes(x = value, fill = class)) +
  geom_histogram(
    bins = 30,
    alpha = 0.6,
    position = "identity"
  ) +
  facet_wrap(~ feature, scales = "free") +
  theme_bw() +
  labs(
    title = "Feature distributions for AMP vs nonAMP",
    x = "Feature value",
    y = "Count"
  )
```

```{r}
##### Correlation Analysis

feature_mat <- feature_df %>%
  select(where(is.numeric))

cor_mat <- cor(feature_mat, method = "pearson")

corrplot(
  cor_mat,
  method = "color",
  type = "upper",
  tl.cex = 0.7
)

high_cor <- which(abs(cor_mat) > 0.9 & abs(cor_mat) < 1, arr.ind = TRUE)

high_cor_pairs <- unique(
  data.frame(
    feature1 = rownames(cor_mat)[high_cor[, 1]],
    feature2 = colnames(cor_mat)[high_cor[, 2]],
    r = cor_mat[high_cor]
  )
)

high_cor_pairs
```

Correlation analysis revealed several highly redundant features, including molecular weight and peptide length (r = 0.99), amphipathicity and hydrophobic moment (r = 1.00), and aliphatic content measures (r \> 0.93). To reduce redundancy and improve interpretability, one feature from each highly correlated pair was removed prior to downstream analyses.

```{r}
# Remove reduntant pairs
feature_df <- feature_df %>%
  select(
    -mw,
    -hmoment,
    -aliphatic_prop,
    -boman_index   
  )

feature_mat_red <- feature_df %>%
  select(where(is.numeric))

pca_red <- prcomp(feature_mat_red, scale. = TRUE)


pca_df_red <- data.frame(
  PC1 = pca_red$x[, 1],
  PC2 = pca_red$x[, 2],
  class = feature_df$class
)

ggplot(pca_df_red, aes(PC1, PC2, color = class)) +
  geom_point(alpha = 0.6) +
  theme_bw() +
  labs(title = "PCA")
```

Initial Model Building

```{r}
# Splitting the data
set.seed(123)

#use 70% of dataset as training set and 30% as test set 
train <- feature_df %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(feature_df, train, by = 'id')
```

Model 1: Logistic Regression

```{r}
"Logistic regression using just 5-10 most promising features
Train on training set
Evaluate on validation set
Metrics: accuracy, sensitivity, specificity, AUC-ROC, MCC"

selected_features <- c(
  "length",
  "charge",
  "hydrophobicity",
  "amphipathicity",
  "aliphatic_index",
  "pI"
)

train_lr <- train[, c("AMP_status", selected_features)]
test_lr  <- test[,  c("AMP_status", selected_features)]

lr_model <- glm(
  AMP_status ~ .,
  data = train_lr,
  family = binomial
)

summary(lr_model)
```

Evaluation of Logistic Regression Model

```{r}
# Predict probabilities on the test set
prob_amp <- predict(lr_model, newdata = test_lr, type = "response")

# Class predictions (0 = nonAMP, 1 = AMP)
pred_class <- ifelse(prob_amp >= 0.5, 1, 0)

# Confusion matrix
cm <- table(
  Predicted = pred_class,
  Actual = test_lr$AMP_status
)

cm

# Accuracy
accuracy <- sum(diag(cm)) / sum(cm)

# Sensitivity (Recall / TPR)
sensitivity <- cm["1", "1"] / sum(cm[, "1"])

# Specificity (TNR)
specificity <- cm["0", "0"] / sum(cm[, "0"])

# MCC
TP <- as.numeric(cm["1", "1"])
TN <- as.numeric(cm["0", "0"])
FP <- as.numeric(cm["1", "0"])
FN <- as.numeric(cm["0", "1"])

mcc <- (TP * TN - FP * FN) /
       sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))

# ROC and AUC
library(pROC)

roc_obj <- roc(
  response = test_lr$AMP_status,
  predictor = prob_amp,
  levels = c(0, 1),
  direction = "<"
)

auc_val <- auc(roc_obj)

plot(roc_obj, main = "Logistic Regression ROC")

# Final summary metrics
lr_metrics <- data.frame(
  Accuracy = accuracy,
  Sensitivity = sensitivity,
  Specificity = specificity,
  AUC = as.numeric(auc_val),
  MCC = mcc
)

lr_metrics


```

Model 2: Random Forest

```{r}
train$class <- factor(train$class, levels = c("nonAMP", "AMP"))
test$class  <- factor(test$class,  levels = c("nonAMP", "AMP"))

# Random Forest
rf <- randomForest(
  class ~ .,
  data = train,
  ntree = 500,
  importance = TRUE
)

pred_class <- predict(rf, newdata = test)
pred_prob <- predict(rf, newdata = test, type = "prob")[, "AMP"]
table(Predicted = pred_class, Actual = test$class)


```

Evaluation of Random Forest

```{r}
# Confusion matrix
rf_cm <- table(Predicted = pred_class, Actual = test$class)
rf_cm

# Extract counts safely
rf_TP <- as.numeric(rf_cm["AMP", "AMP"])
rf_TN <- as.numeric(rf_cm["nonAMP", "nonAMP"])
rf_FP <- as.numeric(rf_cm["AMP", "nonAMP"])
rf_FN <- as.numeric(rf_cm["nonAMP", "AMP"])

# Metrics
rf_accuracy <- (rf_TP + rf_TN) / sum(rf_cm)
rf_sensitivity <- rf_TP / (rf_TP + rf_FN)
rf_specificity <- rf_TN / (rf_TN + rf_FP)

rf_mcc <- (rf_TP * rf_TN - rf_FP * rf_FN) /
       sqrt((rf_TP + rf_FP) * (rf_TP + rf_FN) * (rf_TN + rf_FP) * (rf_TN + rf_FN))

# AUC
rf_roc_obj <- roc(test$class, pred_prob, levels = c("nonAMP", "AMP"))
rf_auc_val <- as.numeric(auc(rf_roc_obj))

# Final summary
rf_metrics <- data.frame(
  Accuracy = rf_accuracy,
  Sensitivity = rf_sensitivity,
  Specificity = rf_specificity,
  AUC = rf_auc_val,
  MCC = rf_mcc
)

rf_metrics
```

Comparison Between Models

```{r}
combined <- bind_rows(
  "Linear Regression" = lr_metrics,
  "Random Forest" = rf_metrics,
  .id = "Model"
)

combined
```

Model Refinement

```{r}

# Ways to improve the RF model
## Add additional features (Individual aa composition, di-amino acid composition (400 features!))
## Tweak model parameters
## Have a more imbalanced dataset? i.e. favouring nonAMP

aa <- c("A","C","D","E","F","G","H","I","K","L",
        "M","N","P","Q","R","S","T","V","W","Y")

for (a in aa) {
  feature_df[[paste0(a, "_prop")]] <-
    str_count(feature_df$sequence, fixed(a)) /
    str_length(feature_df$sequence)
}

# Splitting the data
set.seed(22)

#use 70% of dataset as training set and 30% as test set 
idx <- createDataPartition(feature_df$class, p = 0.7, list = FALSE)

train <- feature_df[idx, ]
test  <- feature_df[-idx, ]

train_rf <- train %>% select(-sequence, -id, -AMP_status)
test_rf  <- test  %>% select(-sequence, -id, -AMP_status)

train_rf$class <- factor(train_rf$class, levels = c("nonAMP", "AMP"))
test_rf$class  <- factor(test_rf$class,  levels = c("nonAMP", "AMP"))

set.seed(22)

x_train <- train_rf %>% select(-class)
y_train <- as.numeric(train_rf$class)

tuned <- tuneRF(
  x = x_train,
  y = y_train,
  ntreeTry = 500,
  stepFactor = 1.5,
  improve = 0.01,
  trace = TRUE
)



```

Final Evaluation

```{r}
set.seed(22)

best_mtry <- tuned[which.min(tuned[, "OOBError"]), "mtry"]
best_mtry

rf <- randomForest(
  class ~ .,
  data = train_rf,
  ntree = 1000,
  mtry = best_mtry,
  classwt = c(nonAMP = 1, AMP = 1),
  importance = TRUE
)
```

```{r}
pred_class <- predict(rf, newdata = test_rf)
pred_prob  <- predict(rf, newdata = test_rf, type = "prob")[, "AMP"]

confusionMatrix(pred_class, test_rf$class)

roc_obj <- roc(
  response = test_rf$class,
  predictor = pred_prob,
  levels = c("nonAMP", "AMP")
)

auc(roc_obj)
plot(roc_obj)

varImpPlot(rf, n.var = 20)
```

Save Model

```{r}
saveRDS(rf, file = "rf_model.rds")
```
